{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an Agentic RAG Pipeline\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/unionai-oss/agentic-rag-workshop/blob/main/agentic-rag.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "In this workshop, we'll build a simple agentic RAG workflow on Union:\n",
    "\n",
    "1. First, we'll create a vector store containing biomedical research documents.\n",
    "2. Then, we'll implement a simple RAG workflow.\n",
    "3. Finally, we'll extend the RAG workflow to be agentic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ Workshop Setup\n",
    "\n",
    "Install python libraries by running the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/unionai-oss/agentic-rag-workshop.git\n",
    "    %cd union-rag\n",
    "    %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the libraries are being installed:\n",
    "- Join the slack channel for support: https://flyte-org.slack.com/archives/C07R0QU6Y2H\n",
    "- Sign up for a free Union account: https://signup.union.ai/\n",
    "- Go to the Union dashboard: https://serverless.union.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login to Union in this notebook session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union create login --auth device-flow --serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔀 Build a Simple Workflow\n",
    "\n",
    "A Union task is a containerized function that takes an input and produces an output. A workflow is a collection of tasks that are executed in a specific sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile simple_wf.py\n",
    "from flytekit import task, workflow\n",
    "\n",
    "@task\n",
    "def hello_world(name: str) -> str:\n",
    "    return f\"Hello, {name}\"\n",
    "\n",
    "@workflow\n",
    "def main(name: str) -> str:\n",
    "    return hello_world(name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run locally\n",
    "!union run simple_wf.py main --name \"Workshop Attendee\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on Union\n",
    "!union run --remote simple_wf.py main --name \"Workshop Attendee\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the main workflow inputs\n",
    "!union run simple_wf.py main --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔑 Create OpenAI API Key Secret on Union\n",
    "\n",
    "First go to https://platform.openai.com/account/api-keys and create an OpenAI API key.\n",
    "\n",
    "Then, run the following command to make the secret accessible on Union:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union create secret openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union get secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have issues with the secret, you can delete it by uncommenting the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!union delete secret openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🗂️ A Simple RAG Workflow\n",
    "\n",
    "> To run, we must first learn how to walk.\n",
    "\n",
    "In the first part of this workshop, we'll build a simple RAG workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./static/rag-workflow.png\" alt=\"Simple RAG Workflow\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an Image Spec\n",
    "\n",
    "An `ImageSpec` is an easy-to-use interface for specifying a container image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile custom_image.py\n",
    "from flytekit import ImageSpec\n",
    "\n",
    "image = ImageSpec(\n",
    "    packages=[\n",
    "        \"beautifulsoup4==4.12.3\",\n",
    "        \"chromadb==0.5.3\",\n",
    "        \"langchain==0.3.2\",\n",
    "        \"langchain-community==0.3.1\",\n",
    "        \"langchain-openai==0.2.2\",\n",
    "        \"langchain-text-splitters==0.3.0\",\n",
    "        \"tiktoken==0.7.0\",\n",
    "        \"xmltodict==0.13.0\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🗃️ Create a Vector Store\n",
    "\n",
    "The first step to doing this is to create a vector store of documents. In the code snippet below, we'll create a vector store of [PubMed](https://pubmed.ncbi.nlm.nih.gov/) documents. The documents will depend on a `query` parameter that we pass into the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vector_store.py\n",
    "from typing import Annotated, Optional\n",
    "\n",
    "from flytekit import task, Deck, Secret\n",
    "from flytekit.deck import MarkdownRenderer\n",
    "from flytekit.types.directory import FlyteDirectory\n",
    "from union.artifacts import Artifact, DataCard\n",
    "\n",
    "from custom_image import image\n",
    "from utils import get_pubmed_loader, parse_doc, generate_data_card, set_openai_api_key\n",
    "\n",
    "\n",
    "# Define the vector store artifact\n",
    "VectorStore = Artifact(name=\"vector-store\")\n",
    "\n",
    "\n",
    "@task(\n",
    "    container_image=image,\n",
    "    cache=True,\n",
    "    cache_version=\"0\",\n",
    "    secret_requests=[Secret(key=\"openai_api_key\")],\n",
    "    enable_deck=True,\n",
    "    deck_fields=[],\n",
    ")\n",
    "def create_vector_store(\n",
    "    query: str,\n",
    "    load_max_docs: Optional[int] = None,\n",
    "    chunk_size: int = 100,\n",
    "    chunk_overlap: int = 50,\n",
    ") -> Annotated[FlyteDirectory, VectorStore]:\n",
    "    \"\"\"Create a vector store of pubmed documents based on a query.\"\"\"\n",
    "\n",
    "    from langchain_community.vectorstores import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "    set_openai_api_key()\n",
    "\n",
    "    load_max_docs = load_max_docs or 10\n",
    "\n",
    "    # load the documents\n",
    "    loader = get_pubmed_loader(\n",
    "        query,\n",
    "        load_max_docs=load_max_docs,\n",
    "        max_retry=200,\n",
    "        sleep_time=1.0,\n",
    "    )\n",
    "    docs = [parse_doc(doc) for doc in loader.load()]\n",
    "\n",
    "    # split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    # create a Chroma vector store\n",
    "    vector_store = Chroma.from_documents(\n",
    "        documents=doc_splits,\n",
    "        collection_name=\"rag-chroma\",\n",
    "        embedding=OpenAIEmbeddings(),\n",
    "        persist_directory=\"./chroma_db\",\n",
    "    )\n",
    "\n",
    "    # create a data card\n",
    "    data_card = generate_data_card(docs)\n",
    "    Deck(\"Data Card\", MarkdownRenderer().to_html(data_card))\n",
    "\n",
    "    return VectorStore.create_from(\n",
    "        FlyteDirectory(path=vector_store._persist_directory),\n",
    "        DataCard(data_card),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector store\n",
    "!union run --remote vector_store.py create_vector_store --query \"CRISPR therapy\" --load_max_docs 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you visit the link produced by the `union run` command, you'll see a task execution that creates the vector store.\n",
    "\n",
    "- Click on the `vector_store.create_vector_store` item in the `Nodes` list view\n",
    "- The right-hand sidebar has a **Flyte Deck** button that shows a preview of the contents of the vector store.\n",
    "- The **Outputs** tab shows the vector store artifact that we created, which we'll use in the next step of this workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔀 Create a RAG Workflow\n",
    "\n",
    "Next, we'll create a simple implementation of a RAG workflow. It will:\n",
    "\n",
    "- Take a user question as an input\n",
    "- Retrieve relevant documents from the vector store\n",
    "- Generate an answer to the user question based on the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile simple_rag.py\n",
    "from typing import Optional\n",
    "\n",
    "from flytekit import workflow, Deck, Resources, Secret\n",
    "from flytekit.deck import MarkdownRenderer\n",
    "from flytekit.types.directory import FlyteDirectory\n",
    "from union.actor import ActorEnvironment\n",
    "from union.artifacts import Artifact\n",
    "\n",
    "from custom_image import image\n",
    "from utils import set_openai_api_key\n",
    "\n",
    "\n",
    "DEFAULT_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks in the biomedical domain.\n",
    "Use only the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. Make the answer as\n",
    "detailed as possible. If the answer contains acronyms, make sure to expand on them.\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "actor = ActorEnvironment(\n",
    "    name=\"simple-rag\",\n",
    "    ttl_seconds=180,\n",
    "    container_image=image,\n",
    "    requests=Resources(cpu=\"2\", mem=\"8Gi\"),\n",
    "    secret_requests=[Secret(key=\"openai_api_key\")],\n",
    ")\n",
    "\n",
    "VectorStore = Artifact(name=\"vector-store\")\n",
    "\n",
    "\n",
    "@actor.task(enable_deck=True, deck_fields=[])\n",
    "def retrieve(\n",
    "    question: str,\n",
    "    vector_store: FlyteDirectory,\n",
    ") -> str:\n",
    "    from langchain_community.vectorstores import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "    set_openai_api_key()\n",
    "\n",
    "    vector_store.download()\n",
    "    vector_store = Chroma(\n",
    "        collection_name=\"rag-chroma\",\n",
    "        persist_directory=vector_store.path,\n",
    "        embedding_function=OpenAIEmbeddings(),\n",
    "    )\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 8},\n",
    "    )\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in retriever.invoke(question))\n",
    "    Deck(\"Context\", MarkdownRenderer().to_html(context))\n",
    "    return context\n",
    "\n",
    "\n",
    "@actor.task(enable_deck=True, deck_fields=[])\n",
    "def generate(\n",
    "    question: str,\n",
    "    context: str,\n",
    "    prompt_template: Optional[str] = None,\n",
    ") -> str:\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "    from langchain_core.prompts import PromptTemplate\n",
    "    from langchain_openai import ChatOpenAI\n",
    "\n",
    "    set_openai_api_key()\n",
    "\n",
    "    prompt = PromptTemplate.from_template(prompt_template or DEFAULT_PROMPT_TEMPLATE)\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0.9)\n",
    "\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    answer = chain.invoke({\"question\": question, \"context\": context})\n",
    "    Deck(\"Answer\", MarkdownRenderer().to_html(answer))\n",
    "    return answer\n",
    "\n",
    "\n",
    "@workflow\n",
    "def run(\n",
    "    question: str,\n",
    "    vector_store: FlyteDirectory = VectorStore.query(),  # 👈 this uses the vector store artifact by default\n",
    "    prompt_template: Optional[str] = None,\n",
    ") -> str:\n",
    "    context = retrieve(question, vector_store)\n",
    "    return generate(\n",
    "        question=question,\n",
    "        context=context,\n",
    "        prompt_template=prompt_template,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the simple RAG workflow\n",
    "!union run --remote simple_rag.py run --question \"What are the latest CRISPR therapies?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The link produced by `union run` will take you to a task execution that runs the simple RAG workflow. Similar to the vector store, we've also created a Flyte Deck that shows the answer to the question we posed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 Making RAG Agentic\n",
    "\n",
    "At a fundamental level, we can talk about Agents as a higher level abstraction: we can define it as an entity that gets a higher level job done by taking a series of automonous actions. For example, you can have a AI agent that helps you order food from a restaurant, or book a flight for you.\n",
    "\n",
    "In this workshop, we'll talk about these kinds of systems at a more fundamental level, i.e. in terms of execution graphs and workflows. An \"agentic\" system is one that uses an AI model (typically a language model) to modify the state of an execution graph, determine the traversal pattern of the graph, or even modify the shape of the graph itself.\n",
    "\n",
    "An \"agentic\" system lies on a spectrum of capabilities, which may be a combination of:\n",
    "\n",
    "1. 🧰 Having access to tools and memory, e.g. vector stores, functions or APIs that an LLM can call.\n",
    "2. 🦾 Producing actions that determines the traversal of the execution graph.\n",
    "3. 🤔 Re-processing the application state through \"self-reflection\" / \"reasoning\".\n",
    "4. 🗓️ Breaking down the job into smaller subtasks through planning.\n",
    "\n",
    "An agentic system can have one or more agents that implement one or more of these capabilities.\n",
    "\n",
    "In this workshop, we'll build an agentic system that implements 1-3 of these capabilities:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./static/agentic-rag-workflow.png\" alt=\"Agentic RAG Workflow\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll define the basic data structures that we'll use to implement agentic RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile agentic_types.py\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class RetrieverAction(Enum):\n",
    "    tools = \"tools\"\n",
    "    end = \"end\"\n",
    "\n",
    "\n",
    "class GraderAction(Enum):\n",
    "    generate = \"generate\"\n",
    "    rewrite = \"rewrite\"\n",
    "    end = \"end\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Message:\n",
    "    \"\"\"Json-encoded message.\"\"\"\n",
    "\n",
    "    data: str\n",
    "\n",
    "    def to_langchain(self):\n",
    "        from langchain_core.messages import AIMessage, ToolMessage, HumanMessage\n",
    "\n",
    "        data = json.loads(self.data)\n",
    "        message_type = data.get(\"type\", data.get(\"role\"))\n",
    "        return {\"ai\": AIMessage, \"tool\": ToolMessage, \"human\": HumanMessage}[message_type](**data)\n",
    "\n",
    "    @classmethod\n",
    "    def from_langchain(cls, message):\n",
    "        return cls(data=json.dumps(message.dict()))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AgentState:\n",
    "    \"\"\"A list of messages capturing the state of the RAG execution graph.\"\"\"\n",
    "\n",
    "    messages: list[Message]\n",
    "\n",
    "    def to_langchain(self) -> dict:\n",
    "        return {\"messages\": [message.to_langchain() for message in self.messages]}\n",
    "\n",
    "    def append(self, message):\n",
    "        self.messages.append(Message.from_langchain(message))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        message: Message = self.messages[index]\n",
    "        return message.to_langchain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we implement the collection of nodes that will form the basis of the agentic RAG workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile agentic_nodes.py\n",
    "from flytekit import Deck, Secret\n",
    "from flytekit.deck import MarkdownRenderer\n",
    "from flytekit.types.directory import FlyteDirectory\n",
    "from union.actor import ActorEnvironment\n",
    "\n",
    "from agentic_types import RetrieverAction, GraderAction, Message, AgentState\n",
    "from custom_image import image\n",
    "from utils import get_vector_store_retriever, set_openai_api_key\n",
    "\n",
    "\n",
    "actor = ActorEnvironment(\n",
    "    name=\"agentic-rag\",\n",
    "    ttl_seconds=180,\n",
    "    container_image=image,\n",
    "    secret_requests=[Secret(key=\"openai_api_key\")],\n",
    ")\n",
    "\n",
    "\n",
    "@actor.task(cache=True, cache_version=\"0\")\n",
    "def init_state(user_message: str) -> AgentState:\n",
    "    \"\"\"Initialize the AgentState with the user's message.\"\"\"\n",
    "    from langchain_core.messages import HumanMessage\n",
    "\n",
    "    return AgentState(messages=[Message.from_langchain(HumanMessage(user_message))])\n",
    "\n",
    "\n",
    "@actor.task\n",
    "def retriever_agent(\n",
    "    state: AgentState,\n",
    "    vector_store: FlyteDirectory,\n",
    ") -> tuple[AgentState, RetrieverAction]:\n",
    "    \"\"\"Invokes the agent to either end the loop or call the retrieval tool.\"\"\"\n",
    "\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain_core.prompts import PromptTemplate\n",
    "    \n",
    "    set_openai_api_key()\n",
    "\n",
    "    vector_store.download()\n",
    "    retriever_tool = get_vector_store_retriever(vector_store.path)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are an biomedical research assistant that can retrieve\n",
    "        documents and answer questions based on those documents.\n",
    "\n",
    "        Here is the user question: {question} \\n\n",
    "\n",
    "        If the question is related to biomedical research, call the relevant\n",
    "        tool that you have access to. If the question is not related to\n",
    "        biomedical research, end the loop with a response that the question\n",
    "        is not relevant.\"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "\n",
    "    question_message = state[-1]\n",
    "    assert question_message.type == \"human\"\n",
    "\n",
    "    model = ChatOpenAI(temperature=0, streaming=True, model=\"gpt-4-turbo\").bind_tools([retriever_tool])\n",
    "    chain = prompt | model\n",
    "    response = chain.invoke({\"question\": question_message.content})\n",
    "\n",
    "    # Get agent's decision to call the retrieval tool or end the loop\n",
    "    action = RetrieverAction.end\n",
    "    if hasattr(response, \"tool_calls\") and len(response.tool_calls) > 0:\n",
    "        action = RetrieverAction.tools\n",
    "\n",
    "    state.append(response)\n",
    "    return state, action\n",
    "\n",
    "\n",
    "@actor.task\n",
    "def retrieve(\n",
    "    state: AgentState,\n",
    "    vector_store: FlyteDirectory,\n",
    ") -> AgentState:\n",
    "    \"\"\"Retrieves documents from the vector store.\"\"\"\n",
    "\n",
    "    from langchain_core.messages import AIMessage, ToolMessage\n",
    "\n",
    "    set_openai_api_key()\n",
    "\n",
    "    vector_store.download()\n",
    "    retriever_tool = get_vector_store_retriever(vector_store.path)\n",
    "\n",
    "    agent_message = state[-1]\n",
    "    assert isinstance(agent_message, AIMessage)\n",
    "    assert len(agent_message.tool_calls) == 1\n",
    "\n",
    "    # invoke the tool to retrieve documents from the vector store\n",
    "    tool_call = agent_message.tool_calls[0]\n",
    "    content = retriever_tool.invoke(tool_call[\"args\"])\n",
    "    response = ToolMessage(content=content, tool_call_id=tool_call[\"id\"])\n",
    "    state.append(response)\n",
    "    return state\n",
    "\n",
    "\n",
    "@actor.task\n",
    "def grader_agent(state: AgentState) -> GraderAction:\n",
    "    \"\"\"Determines whether the retrieved documents are relevant to the question.\"\"\"\n",
    "\n",
    "    from langchain_core.prompts import PromptTemplate\n",
    "    from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "    from langchain_openai import ChatOpenAI\n",
    "\n",
    "    set_openai_api_key()\n",
    "\n",
    "    # Restrict the LLM's output to be a binary \"yes\" or \"no\"\n",
    "    class Grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM with tool and validation\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n",
    "    llm = model.with_structured_output(Grade)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved \n",
    "        document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the\n",
    "        user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the\n",
    "        document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm\n",
    "\n",
    "    messages = state.to_langchain()[\"messages\"]\n",
    "\n",
    "    # get the last \"human\" and \"tool\" message, which contains the question and\n",
    "    # retrieval tool context, respectively\n",
    "    questions = [m for m in messages if m.type == \"human\"]\n",
    "    contexts = [m for m in messages if m.type == \"tool\"]\n",
    "    question = questions[-1]\n",
    "    context = contexts[-1]\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question.content, \"context\": context.content})\n",
    "    score = scored_result.binary_score\n",
    "    return {\n",
    "        \"yes\": GraderAction.generate,\n",
    "        \"no\": GraderAction.rewrite,\n",
    "    }[score]\n",
    "\n",
    "\n",
    "@actor.task\n",
    "def rewrite(state: AgentState) -> AgentState:\n",
    "    \"\"\"Transform the query to produce a better question.\"\"\"\n",
    "\n",
    "    from langchain_core.messages import HumanMessage\n",
    "    from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "    from langchain_openai import ChatOpenAI\n",
    "\n",
    "    set_openai_api_key()\n",
    "\n",
    "    messages = state.to_langchain()[\"messages\"]\n",
    "\n",
    "    # get the last \"human\", which contains the user question\n",
    "    questions = [m for m in messages if m.type == \"human\"]\n",
    "    question = questions[-1].content\n",
    "\n",
    "    class rewritten_question(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        question: str = Field(description=\"Rewritten question\")\n",
    "        reason: str = Field(description=\"Reasoning for the rewrite\")\n",
    "\n",
    "    rewrite_prompt = f\"\"\"\n",
    "    Look at the input and try to reason about the underlying semantic\n",
    "    intent / meaning. \\n\n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question} \n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question and provide your reasoning.\n",
    "    \"\"\"\n",
    "\n",
    "    # define model with structured output for the question rewrite\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n",
    "    rewriter_model = model.with_structured_output(rewritten_question)\n",
    "\n",
    "    response = rewriter_model.invoke([HumanMessage(content=rewrite_prompt)])\n",
    "    message = HumanMessage(\n",
    "        content=response.question,\n",
    "        response_metadata={\"rewrite_reason\": response.reason},\n",
    "    )\n",
    "    state.append(message)\n",
    "    return state\n",
    "\n",
    "\n",
    "@actor.task\n",
    "def generate(state: AgentState) -> AgentState:\n",
    "    \"\"\"Generate an answer based on the state.\"\"\"\n",
    "\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain_core.messages import AIMessage\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "    set_openai_api_key()\n",
    "\n",
    "    messages = state.to_langchain()[\"messages\"]\n",
    "\n",
    "    # get the last \"human\" and \"tool\" message, which contains the question and\n",
    "    # retrieval tool context, respectively\n",
    "    questions = [m for m in messages if m.type == \"human\"]\n",
    "    contexts = [m for m in messages if m.type == \"tool\"]\n",
    "    question = questions[-1]\n",
    "    context = contexts[-1]\n",
    "\n",
    "    system_message = \"\"\"\n",
    "    You are an assistant for question-answering tasks in the biomedical domain.\n",
    "    Use the following pieces of retrieved context to answer the question. If you\n",
    "    don't know the answer, just say that you don't know. Make the answer as\n",
    "    detailed as possible. If the answer contains acronyms, make sure to expand\n",
    "    them.\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([(\"human\", system_message)])\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, streaming=True)\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    response = rag_chain.invoke({\"context\": context.content, \"question\": question.content})\n",
    "    if isinstance(response, str):\n",
    "        response = AIMessage(response)\n",
    "\n",
    "    state.append(response)\n",
    "    return state\n",
    "\n",
    "\n",
    "@actor.task(enable_deck=True, deck_fields=[])\n",
    "def return_answer(state: AgentState) -> str:\n",
    "    \"\"\"Finalize the answer to return a string to the user.\"\"\"\n",
    "\n",
    "    if len(state.messages) == 1:\n",
    "        answer = f\"I'm sorry, I don't understand: '{state.messages}'\"\n",
    "    else:\n",
    "        data = state.messages[-1].to_langchain()\n",
    "        answer = data.content\n",
    "    Deck(\"Answer\", MarkdownRenderer().to_html(answer))\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we put everything together into a single workflow.\n",
    "\n",
    "We're going to define a `retrieval_router` and `rewrite_or_generate_router` to implement the conditional branching logic that we saw in the workflow diagram earlier. This allows the agent node actions to effect the flow of execution.\n",
    "\n",
    "Then, we wrap it all into a `run` workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile agentic_rag.py\n",
    "from flytekit import dynamic, workflow, Secret\n",
    "from flytekit.types.directory import FlyteDirectory\n",
    "from union.actor import ActorEnvironment\n",
    "from union.artifacts import Artifact\n",
    "\n",
    "from agentic_types import RetrieverAction, GraderAction, AgentState\n",
    "from agentic_nodes import init_state, retriever_agent, retrieve, grader_agent, rewrite, generate, return_answer\n",
    "from custom_image import image\n",
    "\n",
    "\n",
    "actor = ActorEnvironment(\n",
    "    name=\"agentic-rag\",\n",
    "    ttl_seconds=180,\n",
    "    container_image=image,\n",
    "    secret_requests=[Secret(key=\"openai_api_key\")],\n",
    ")\n",
    "\n",
    "VectorStore = Artifact(name=\"vector-store\")\n",
    "\n",
    "MAX_REWRITES = 10  # 👈 maximum number of question rewrites\n",
    "\n",
    "\n",
    "@dynamic\n",
    "def retrieval_router(\n",
    "    state: AgentState,\n",
    "    action: RetrieverAction,\n",
    "    vector_store: FlyteDirectory,\n",
    "    n_rewrites: int,\n",
    ") -> AgentState:\n",
    "    \"\"\"\n",
    "    The first conditional branch in the RAG workflow. This determines whether\n",
    "    the execution graph should end or call the retrieval tool for grading.\n",
    "    \"\"\"\n",
    "\n",
    "    if action == RetrieverAction.end:\n",
    "        return state\n",
    "    elif action == RetrieverAction.tools:\n",
    "        state = retrieve(state=state, vector_store=vector_store)\n",
    "        grader_action = grader_agent(state=state)\n",
    "        return rewrite_or_generate_router(state, grader_action, vector_store, n_rewrites)\n",
    "    else:\n",
    "        raise RuntimeError(f\"Invalid action '{action}'\")\n",
    "\n",
    "\n",
    "@dynamic\n",
    "def rewrite_or_generate_router(\n",
    "    state: AgentState,\n",
    "    grader_action: GraderAction,\n",
    "    vector_store: FlyteDirectory,\n",
    "    n_rewrites: int,\n",
    ") -> AgentState:\n",
    "    \"\"\"\n",
    "    The second conditional branch in the RAG workflow. This determines whether\n",
    "    the rewrite the original user's query or generate the final answer.\n",
    "    \"\"\"\n",
    "    if grader_action == GraderAction.generate or n_rewrites >= MAX_REWRITES:\n",
    "        return generate(state=state)\n",
    "    elif grader_action == GraderAction.rewrite:\n",
    "        state = rewrite(state=state)\n",
    "        state, action = retriever_agent(state=state, vector_store=vector_store)\n",
    "        n_rewrites += 1\n",
    "        return retrieval_router(\n",
    "            state=state,\n",
    "            action=action,\n",
    "            vector_store=vector_store,\n",
    "            n_rewrites=n_rewrites,\n",
    "        )\n",
    "    else:\n",
    "        raise RuntimeError(f\"Invalid action '{grader_action}'\")\n",
    "\n",
    "\n",
    "\n",
    "@workflow\n",
    "def run(\n",
    "    question: str,\n",
    "    vector_store: FlyteDirectory = VectorStore.query(),\n",
    ") -> str:\n",
    "    \"\"\"An agentic retrieval augmented generation workflow.\"\"\"\n",
    "    state = init_state(user_message=question)\n",
    "    state, action = retriever_agent(state=state, vector_store=vector_store)\n",
    "    state = retrieval_router(state, action, vector_store, n_rewrites=0)\n",
    "    return return_answer(state=state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the simple RAG workflow\n",
    "!union run --remote agentic_rag.py run --question \"What are the latest CRISPR therapies?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger the workflow to end early\n",
    "!union run --remote agentic_rag.py run --question \"What's the weather in Atlanta today\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger the workflow to rewrite the question\n",
    "!union run --remote agentic_rag.py run --question \"Wat are CRESPR treatment?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎉 Congratulations! You've completed the workshop.\n",
    "\n",
    "You've learned about:\n",
    "- The basic structure and requirements of a RAG workflow.\n",
    "- How to create a vector store for contextual retrieval.\n",
    "- How to implement a simple RAG workflow.\n",
    "- How to make a RAG workflow agentic by implementing:\n",
    "  - Decision-making nodes\n",
    "  - Tools\n",
    "  - Application state updates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Exercise: Implement an RAG workflow with a web search tool\n",
    "\n",
    "If you want to test your knowledge of agentic RAG, try implementing a RAG workflow that uses a web search tool. It should take the output of the retrieval tool, reason about the relevance of the retrieved context relative to the question, and produce two actions:\n",
    "\n",
    "- `websearch`: call the web search tool and use that as the context instead of the retrieved documents from the vector store.\n",
    "- `passthrough`: pass through the retrieved documents from the vector store as the context.\n",
    "\n",
    "Here's the workflow at a high level:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./static/agentic-rag-with-search.png\" alt=\"Agentic RAG Workflow with Web Search\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution goes here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-rag-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
